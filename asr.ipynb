{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR: using pyaudio and ... whisper is to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- New a pyaudio object and find the api index we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyaudio\n",
    "import time\n",
    "import wave\n",
    "import matplotlib.pyplot as plt\n",
    "# import whisper\n",
    "from faster_whisper import WhisperModel\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load asr model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model = WhisperModel('medium', device='cuda', compute_type=\"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find mic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pyaudio.PyAudio()\n",
    "def find_api(audio:pyaudio.PyAudio, name) -> int:\n",
    "    api_cnt = audio.get_host_api_count()\n",
    "    for i in range(api_cnt):\n",
    "        inf = audio.get_host_api_info_by_index(i)\n",
    "        for key in inf.keys():\n",
    "            if key=='name':\n",
    "                if inf[key]==name:\n",
    "                    return i\n",
    "    return -1\n",
    "apii = find_api(audio=p, name='MME')\n",
    "def find_mic_in_api(audio:pyaudio.PyAudio, apii)->dict:\n",
    "    dc = audio.get_host_api_info_by_index(apii)['deviceCount']\n",
    "    mic_in_words = '麥克風'\n",
    "    ret = None\n",
    "    for i in range(dc):\n",
    "        dv = audio.get_device_info_by_host_api_device_index(apii, i)\n",
    "        indx = int(dv['index'])\n",
    "        hstApi=int(dv['hostApi'])\n",
    "        inpuCh=int(dv['maxInputChannels'])\n",
    "        smplRa=int(dv['defaultSampleRate'])\n",
    "        name=dv['name']\n",
    "        if inpuCh>0:\n",
    "            if mic_in_words in name:\n",
    "              ret = {'index': i, 'input channels': inpuCh, 'sample rate': smplRa, 'name': name}\n",
    "              break\n",
    "    return ret\n",
    "devi = find_mic_in_api(audio=p, apii=apii)\n",
    "if not devi:\n",
    "    print(f\"Panic! Could not find mic!\")\n",
    "    exit(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stream callback and VU calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vu(data)->int:\n",
    "    x = np.frombuffer(data, dtype=np.int16).astype('i4')  # Convert from int16 to int32\n",
    "    rms = np.sqrt(np.mean(np.square(x)))  # Calculate RMS\n",
    "    vu = 20 * np.log10(rms)  # Convert RMS to VU (decibels)\n",
    "    return int(vu)\n",
    "# Stream callback function\n",
    "def process(data, fc, tim_inf, flag):\n",
    "    global to_abort\n",
    "    global frms\n",
    "    global vus\n",
    "    global dat_q\n",
    "    vu = calculate_vu(data=data)\n",
    "    vus.append(vu)\n",
    "    dat_q.put(data)\n",
    "    if to_abort == False:  # 150 counts are about 3sec\n",
    "        return (None, pyaudio.paContinue)\n",
    "    else:\n",
    "        print(f\"To abort. \")\n",
    "        return (None, pyaudio.paAbort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Open stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_abort = False\n",
    "frms = list()\n",
    "vus = list()\n",
    "dat_q = Queue()\n",
    "stream = p.open(\n",
    "    format=pyaudio.paInt16,\\\n",
    "    channels = devi['input channels'],\\\n",
    "    rate=16000,\\\n",
    "    frames_per_buffer=1024,\\\n",
    "    input=True,\\\n",
    "    input_device_index=devi['index'],\\\n",
    "    stream_callback=process,\\\n",
    "    start=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long time loop started, press ctrl + c to stop.\n",
      "To abort. \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    stream.start_stream()\n",
    "    print(f\"Long time loop started, press ctrl + c to stop.\")\n",
    "    i = 0\n",
    "    while stream.is_active():\n",
    "        time.sleep(1.5)\n",
    "        if not dat_q.empty():\n",
    "            aud_dat = b''.join(dat_q.queue)\n",
    "            dat_q.queue.clear()\n",
    "            frms.append(aud_dat)\n",
    "            # np_audio = np.frombuffer(aud_dat, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "            # result = asr_model.transcribe(np_audio, fp16=True)\n",
    "            # text = result['text'].strip()\n",
    "            # print(text)\n",
    "        i += 1\n",
    "        if i > 5:\n",
    "            to_abort = True\n",
    "            time.sleep(1.0)\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    to_abort = True\n",
    "finally:\n",
    "    time.sleep(1.0)\n",
    "    stream.stop_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.array(vus)\n",
    "# plt.plot(x)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WhisperModel.transcribe() got an unexpected keyword argument 'fp16'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m np_audio \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(frms), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint16)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m32768.0\u001b[39m\n\u001b[1;32m----> 2\u001b[0m segments, info \u001b[38;5;241m=\u001b[39m \u001b[43masr_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m segment \u001b[38;5;129;01min\u001b[39;00m segments:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(segment\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[1;31mTypeError\u001b[0m: WhisperModel.transcribe() got an unexpected keyword argument 'fp16'"
     ]
    }
   ],
   "source": [
    "np_audio = np.frombuffer(b''.join(frms), dtype=np.int16).astype(np.float32) / 32768.0\n",
    "segments, info = asr_model.transcribe(np_audio, language='zh')\n",
    "for segment in segments:\n",
    "    print(segment.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write frames to wav file and close everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = wave.open(\".\\\\to.wav\",'wb')\n",
    "w.setnchannels(1)\n",
    "w.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "w.setframerate(16000)\n",
    "w.writeframes(b''.join(frms))\n",
    "\n",
    "w.close()\n",
    "stream.close()\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- More example: https://github.com/davabase/whisper_real_time/blob/master/transcribe_demo.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
